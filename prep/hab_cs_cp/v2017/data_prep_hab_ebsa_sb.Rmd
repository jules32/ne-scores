---
title: 'OHIBC: Habitat goal - EBSA layer prep'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/ohibc/src/templates/ohibc_hdr1.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(raster)
library(rgdal)

source('~/github/ohibc/src/R/common.R')  ### an OHIBC specific version of common.R

dir_git     <- '~/github/ohibc'
dir_spatial <- file.path(dir_git, 'prep/spatial')  ### github: general buffer region shapefiles
dir_anx     <- file.path(dir_M, 'git-annex/bcprep')

### goal specific folders and info
goal      <- 'hab_cs_cp'
scenario  <- 'v2017'
dir_goal  <- file.path(dir_git, 'prep', goal, scenario)
dir_goal_anx <- file.path(dir_anx, goal, scenario)

dir_fis   <- file.path(dir_anx, '_raw_data/dfo_khunter/d2016/fisheries')
dir_trawl <- file.path(dir_goal_anx, 'trawl')
dir_habs  <- file.path(dir_goal_anx, 'habs')

### provenance tracking
library(provRmd); prov_setup()

### support scripts
# source(file.path('~/github/ohibc/src/R/map_scores.R'))
  ### score plotting scripts
source(file.path(dir_git, 'src/R/rast_tools.R'))
  ### raster plotting and analyzing scripts

### set up the default BC projection to be BC Albers
p4s_bcalb <- c('bcalb' = '+init=epsg:3005')

### set up base raster/region raster file
rast_base_file <- file.path(dir_spatial, 'raster/ohibc_rgn_raster_500m.tif')

```

# Summary

Create habitat info layer for EBSAs - ecologically and biologically significant areas.  The goal will be calculated based upon trawl pressure in the EBSAs.

# Updates from previous assessment

***

# Data Source [NOTE: can be copied from README.md in rawdata file]
**Reference**: [citation for source data; website, literature, contact information. Version of data (if relevant). Screenshots if a series of menus had to be navigated to obtain the data.]

**Downloaded**: [date downloaded or received]

**Description**:  [e.g., surface aragonite state]

**Native data resolution**: [e.g., 1 degree, 30 m, etc.]   

**Time range**: [e.g., 1880-1899, monthly data provided for each year] 

**Format**:  [e.g. NetCDF]

***
  
# Methods

- EBSA layers as static spatial extent to inform pressure-based models
- EBSA rasters will be created at 500 m resolution; should be adequate since features are large
    - stored on git-annex
    
``` {r create_OHI_EBSA_shp}

poly_ebsa <- readOGR(dsn = file.path(dir_anx, '_raw_data/dfo_khunter/d2016/management_boundaries/ebsa'), 
                     layer = 'eco_bio_sig_areas', 
                     stringsAsFactors = FALSE)

ebsa_sar_2012 <- read_csv(file.path(dir_goal, 'raw/ebsa_sar_2012.csv'))

poly_ebsa@data <- poly_ebsa@data %>% 
  setNames(tolower(names(.))) %>%
  left_join(ebsa_sar_2012, by = c('id' = 'ebsa_id')) %>%
  select(ebsa_id = id, name, label, phys = physical, unique, agg = aggregation, conf) %>%
  mutate(agg  = ifelse(!is.na(unique), unique, agg),
         phys = ifelse(str_detect(label, 'Seamount'), 'seamount', phys),
         phys = ifelse(str_detect(label, 'Hydrothermal'), 'hydrothermal', phys),
         hab  = (agg %in% c('corals', 'sponges') | 
                   phys %in% c('seamount', 'hydrothermal')),
         ebsa_id = ifelse(ebsa_id == 0, 200, ebsa_id)) %>%
  select(-unique, -label)

poly_ebsa_trim <- poly_ebsa[poly_ebsa@data$hab == TRUE, ]

# Note: Strait of Georgia sponge reefs not in main EBSA polygons; use
# sponge reef closures layer to capture these.  
poly_sg_sponges <- readOGR(dsn = file.path(dir_anx, '_raw_data/dfo_khunter/d2016',
                                           'management_boundaries/mgmt_related_boundaries'), 
                     layer = 'Strait_of_Georgia_Sponge_Reef_Fishing_Closures', 
                     stringsAsFactors = FALSE)
poly_sg_sponges@data <- poly_sg_sponges@data %>%
  mutate(ebsa_id   = OBJECTID + 100,
         name = 'Sponge Reef',
         phys = NA,
         agg  = 'sponges',
         conf = NA,
         hab  = TRUE) %>%
  select(-OBJECTID, -Shape_Leng, -Shape_Area)

# plot(poly_ebsa_trim)
# plot(poly_sg_sponges, border = 'red', add = TRUE)

### Need to re-ID the polygons to prep for rbind...
poly_ebsa_trim  <- poly_ebsa_trim %>% spChFIDs(as.character(1:nrow(poly_ebsa_trim)))
poly_sg_sponges <- poly_sg_sponges %>% spChFIDs(as.character(1:nrow(poly_sg_sponges) + nrow(poly_ebsa_trim)))
poly_ebsa_hab <- rbind(poly_ebsa_trim, poly_sg_sponges, makeUniqueIDs = TRUE)

### save locally for next step: rasterization
writeOGR(poly_ebsa_hab, dsn = path.expand(dir_habs),
         layer = 'ebsa_hab', driver = 'ESRI Shapefile',
         overwrite_layer = TRUE)

```

``` {r select_and_rasterize_ebsas}

base_rast <- raster(rast_base_file)

rast_ebsa_hab <- gdal_rast2(src = file.path(dir_habs, 'ebsa_hab'),
           base_rast,
           dst = file.path(dir_habs, 'ebsa_hab.tif'),
           value = 'ebsa_id',
           override_p4s = TRUE)
           
```

***

OK, now to get all the trawl pressures layers

* Groundfish trawl (use new trawl data only?)
    * `Sum_EFFORT` field - effort in hours(?) across 4 km x 4 km cell (16 km^2)
    * 2007-2015
* Scallop trawl
    * `Sum_tm_fsh` field - effort in trawl hours(?) across 4 km x 4 km cell (16 km^2)
    * 2005-2015; ***2010 data seems corrupted so is dropped***
* Shrimp trawl
    * is this bottom trawl? http://thisfish.info/fishery/shrimp-bottom-trawl-british-columbia/
    * `sum_tow_ti` field - effort in total tow time? across 4 km x 4 km cell (16 km^2)
    * 2005-2015; ***No shapefile for 2014?***

``` {r create_groundfish_trawl_effort_layers}

trawl_gf_polys <- list.files(file.path(dir_fis, 'groundfish_trawl_2007_2015'),
                             pattern = '.shp$', full.names = TRUE)
trawl_gf_rasts <- file.path(dir_trawl,
                            basename(trawl_gf_polys) %>%
                              str_replace('GFTrawl', 'groundfish_trawl') %>%
                              str_replace('_legal_limfields', '') %>%
                              str_replace('.shp$', '.tif'))
# x <- lapply(trawl_gf_polys %>% str_replace('.shp', '.dbf'), foreign::read.dbf) %>% setNames(basename(trawl_gf_polys))
# y <- lapply(x, names) %>% setNames(basename(trawl_gf_polys))
### All shapefiles consistent:
### [1] "Count_"     "Sum_CATCH_" "Sum_EFFORT" "Shape_Leng" "Shape_Le_1" "Shape_Area"


rast_base <- raster(rast_base_file)

if(any(!file.exists(trawl_gf_rasts))) {

  tmp <- parallel::mclapply(trawl_gf_polys, mc.cores = 12, FUN = function(poly_file) {
    # poly_file <- trawl_gf_polys[1]
    new_rast_file <- file.path(dir_trawl,
                               basename(poly_file) %>%
                                 str_replace('GFTrawl', 'groundfish_trawl') %>%
                                 str_replace('_legal_limfields', '') %>%
                                 str_replace('.shp$', '.tif'))
    
    gdal_rast2(src = poly_file,
               rast_base,
               dst = new_rast_file,
               override_p4s = TRUE,
               value = 'Sum_EFFORT')
  })
} 

### force git provenance registration
git_prov(trawl_gf_polys, filetype = 'input')
git_prov(trawl_gf_rasts, filetype = 'output')

```

``` {r create_scallop_trawl_effort_layers}

trawl_scallop_polys <- list.files(file.path(dir_fis, 'scallop_trawl_2005_2015'),
                             pattern = '.shp$', full.names = TRUE)
# x <- lapply(trawl_scallop_polys %>% str_replace('.shp', '.dbf'), foreign::read.dbf) %>% setNames(basename(trawl_scallop_polys))
# y <- lapply(x, names) %>% setNames(basename(trawl_scallop_polys))
# $Scalloptrawl_2005_legal_limfields.shp
# [1] "OBJECTID_1" "Count_"     "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2006_legal_limfields.shp
# [1] "OBJECTID_1" "Count_"     "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2007_legal_limfields.shp
# [1] "OBJECTID_1" "Count_"     "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2008_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2009_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2010_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_OBJECT" "Sum_ET_ID"  "Sum_Count_" "Sum_Sum_co" "Sum_Sum_cf" "Sum_Sum_ye" "Sum_Sum_ge"
# [12] "Sum_Sum_mo" "Sum_Sum_da" "Sum_Sum_ho" "Sum_Sum_st" "Sum_Sum_su" "Sum_Sum_mx" "Sum_Sum_mn" "Sum_Sum_nu" "Sum_Sum_tm" "Sum_Sum_tt" "Sum_Sum_la"
# [23] "Sum_Sum__1" "Sum_Sum_lo" "Sum_Sum__2" "Sum_Sum_pa" "Sum_Sum_li" "Sum_Sum_Ds" "Sum_Shape_" "Sum_Shape1" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2011_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2012_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2013_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2014_legal_limfields.shp
#  [1] "OBJECTID"   "ET_ID"      "ET_Index"   "Count_"     "Sum_counte" "Sum_cfv"    "Sum_year"   "Sum_gear_c" "Sum_month"  "Sum_day"    "Sum_hour"  
# [12] "Sum_stat_a" "Sum_sub_ar" "Sum_mx_dpt" "Sum_mn_dpt" "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Sum_lat_de" "Sum_lat_mi" "Sum_long_d" "Sum_long_m"
# [23] "Sum_page_n" "Sum_line_n" "Sum_DscrdW" "Shape_Leng" "Shape_Area"
# 
# $Scalloptrawl_2015_legal_limfields.shp
# [1] "OBJECTID_1" "Count_"     "Sum_num_tw" "Sum_tm_fsh" "Sum_tt_lnd" "Shape_Leng" "Shape_Area"

### Use sum_tm_fsh for now: "sum time fished"?  NOTE: column names in 2010
### data are problematic, and data looks questionable.  Drop from analysis.
trawl_scallop_polys <- trawl_scallop_polys[!str_detect(trawl_scallop_polys, '2010')]

trawl_scallop_rasts <- file.path(dir_trawl,
                            basename(trawl_scallop_polys) %>%
                              str_replace('Scalloptrawl', 'scallop_trawl') %>%
                              str_replace('_legal_limfields', '') %>%
                              str_replace('.shp$', '.tif'))

rast_base <- raster(rast_base_file)

if(any(!file.exists(trawl_scallop_rasts))) {

  tmp <- parallel::mclapply(trawl_scallop_polys, mc.cores = 12, FUN = function(poly_file) {
    # poly_file <- trawl_scallop_polys[1]
    new_rast_file <- file.path(dir_trawl,
                               basename(poly_file) %>%
                                 str_replace('Scalloptrawl', 'scallop_trawl') %>%
                                 str_replace('_legal_limfields', '') %>%
                                 str_replace('.shp$', '.tif'))
    
    gdal_rast2(src = poly_file,
               rast_base,
               dst = new_rast_file,
               override_p4s = TRUE,
               value = 'Sum_tm_fsh')
  })
}

### force git provenance registration
git_prov(trawl_scallop_polys, filetype = 'input')
git_prov(trawl_scallop_rasts, filetype = 'output')

```

``` {r create_shrimp_trawl_effort_layers}

trawl_shrimp_polys <- list.files(file.path(dir_fis, 'shrimp_trawl_2005_2015'),
                             pattern = '.shp$', full.names = TRUE)
trawl_shrimp_rasts <- file.path(dir_trawl,
                            basename(trawl_gf_polys) %>%
                              str_replace('Shrimptrawl', 'shrimp_trawl') %>%
                              str_replace('_legal_limfields', '') %>%
                              str_replace('.shp$', '.tif'))
# x <- lapply(trawl_shrimp_polys %>% str_replace('.shp', '.dbf'), foreign::read.dbf) %>% setNames(basename(trawl_shrimp_polys))
# y <- lapply(x, names) %>% setNames(basename(trawl_shrimp_polys))
# $Shrimptrawl_2005_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2006_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w" "Sum_spny_p" "Shape_Leng"
# [12] "Shape_Area"
# 
# $Shrimptrawl_2007_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2008_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2009_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2010_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $ShrimpTrawl_2011_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2012_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $Shrimptrawl_2013_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
# 
# $ShrimpTrawl_2015_legal_limfields.shp
#  [1] "OBJECTID_1" "Count_"     "Sum_tow_ti" "Sum_pinks"  "Sum_sdstrp" "Sum_prawns" "Sum_hmpbck" "Sum_docks"  "Sum_smth_p" "Sum_flxd_p" "Sum_othr_w"
# [12] "Sum_spny_p" "Shape_Leng" "Shape_Area"
### Reasonably consistent: use Sum_tow_ti ("Sum tow time"?)
### NOTE: no 2014 data?

rast_base <- raster(rast_base_file)

if(any(!file.exists(trawl_shrimp_rasts))) {

  tmp <- parallel::mclapply(trawl_shrimp_polys, mc.cores = 12, FUN = function(poly_file) {
    # poly_file <- trawl_shrimp_polys[1]
    new_rast_file <- file.path(dir_trawl,
                               basename(tolower(poly_file)) %>%
                                 str_replace('shrimptrawl', 'shrimp_trawl') %>%
                                 str_replace('_legal_limfields', '') %>%
                                 str_replace('.shp$', '.tif'))
    
    gdal_rast2(src = poly_file,
               rast_base,
               dst = new_rast_file,
               override_p4s = TRUE,
               value = 'Sum_tow_ti')
  })
} 

### force git provenance registration
git_prov(trawl_shrimp_polys, filetype = 'input')
git_prov(trawl_shrimp_rasts, filetype = 'output')

```

***

### sum trawl pressures rasters - assuming all effort in hours.

``` {r sum_pressures_and_mask}
trawl_rast_files <- list.files(file.path(dir_trawl),
                             pattern = 'trawl', full.names = TRUE)
trawl_rast_files <- trawl_rast_files[!str_detect(trawl_rast_files, 'total')]
trawl_rast_files <- trawl_rast_files[str_detect(trawl_rast_files, '.tif$')]

trawl_rasts <- lapply(trawl_rast_files, raster) %>%
  setNames(basename(trawl_rast_files) %>% str_replace('.tif', ''))

trawl_years_df <- data.frame('x' = names(trawl_rasts),
                    stringsAsFactors = FALSE) %>%
  separate(x, c('fis', 'year'), sep = '_trawl_')

### initialize list of trawl totals
trawl_years  <- unique(trawl_years_df$year)
trawl_layers <- paste0('trawl_totals_', trawl_years)

reload <- FALSE

if(any(!file.exists(file.path(dir_trawl, paste0(trawl_layers, '.tif'))) | reload)) {
  trawl_totals <- vector('list', length = length(trawl_years)) %>%
    setNames(trawl_layers)
  
  ### loop over each year, make stack of all trawl layers for that year,
  ### then sum them (with na.rm = TRUE)
  for(i in 1:length(trawl_years)) { # i = 3
    year_layers <- str_detect(names(trawl_rasts), as.character(trawl_years[i]))
    trawl_totals[[trawl_layers[i]]] <- trawl_rasts[year_layers] %>% 
      stack() %>% 
      calc(sum, na.rm = TRUE)
  }
  
  trawl_totals <- stack(trawl_totals)
  
  ### adjust to be per .5 x .5 km cell (raw values are per 4 km x 4 km cell)
  trawl_totals <- trawl_totals * .25^2 * .5^2
  
  writeRaster(trawl_totals, bylayer = TRUE,
              filename  = file.path(dir_trawl, paste0(trawl_layers, '.tif')),
              overwrite = TRUE)
} else {
  
  git_prov(file.path(dir_trawl, paste0(trawl_layers, '.tif')), filetype = 'output')
  
}

```

***

## Calculate trawl pressure on EBSAs (binary method)

For EBSAs, calculate trawl pressure as any trawl activity in a cell - binary method.  This represents the long-term damage caused by trawling on these fragile and slow-growing ecosystems (corals, seamounts, sponge reefs, hydrothermal vents).  The reference point would be zero trawl effort in all EBSA areas.  The region ID, EBSA area, and trawled area by year are saved to the output folder.

``` {r crosstab_ebsa_habs_to_rgns_binary}

ebsa_trawl_rgn_file <- file.path(dir_goal, 'output', 'hab_ebsa_trawl.csv')

trawl_total_files <- list.files(file.path(dir_trawl), 
                               pattern = 'trawl_totals',
                               full.names = TRUE)

ebsa_rast <- raster(file.path(dir_habs, 'ebsa_hab.tif'))

### now crosstab ebsa trawl effort by regions

trawl_totals <- lapply(trawl_total_files, raster) %>%
  setNames(basename(trawl_total_files) %>% str_replace('.tif$', ''))
  ### need a list or vector for mclapply; stack is treated as one obj?

if(!file.exists(ebsa_trawl_rgn_file) | reload) {
  
  message(ebsa_trawl_rgn_file, ' does not exist or else reload == TRUE')
  trawl_totals_bin <- parallel::mclapply(trawl_totals, mc.cores = 12,
                                         FUN = function(rast) {
    rast1 <- rast ### set new raster without affecting original
    values(rast1)[values(rast) > 0] <- 1
    ### using this method, zeros stay zero instead of NaN.  Zeros
    ### indicate EBSA area with no trawl pressure, so leave 'em in
    names(rast1) <- names(rast) ### the recalc turns raster name into "layer"
    return(rast1)
  })
  
  ### Need to re-mask by the EBSA areas, since the na.rm = TRUE call above
  ### will turn all NA cells (non-EBSA areas) into zeros.
  ebsa_trawl_rasts_bin <- trawl_totals_bin %>%
    parallel::mclapply(FUN = function(trawl_layer) {
                         mask(trawl_layer, ebsa_rast)
                         }, 
                       mc.cores = 12)
  
  rgn_rast <- raster(rast_base_file) %>%
    setNames('rgn_id')
  
  ebsa_trawl_rgn_list <- ebsa_trawl_rasts_bin %>%
    parallel::mclapply(mc.cores = 12,
      FUN = function(rast) { ### rast <- ebsa_trawl_rasts_bin[[1]]
        trawl_year <- names(rast) %>%
          str_replace('trawl_totals_', '')
  
        rast <- rast %>%
          setNames('trawl_effort')
        message('Crosstabulating ', names(rast), ' for ', trawl_year)
        trawl_df <- raster::crosstab(rast, rgn_rast,
                                    digits = 0,
                                    long = TRUE,
                                    useNA = TRUE,
                                    progress = 'text')
        trawl_df <- trawl_df %>%
          mutate(trawl_effort = as.numeric(as.character(trawl_effort)), ### dammit factors!
                 year = as.integer(trawl_year))
      })
    
  message('binding cross-tabbed dataframes from each year of trawl effort')
  ebsa_trawl_rgn_df <- bind_rows(ebsa_trawl_rgn_list) %>%
    filter(!is.na(rgn_id) & !is.na(trawl_effort)) %>% ### exclude non-EBSA cells
    group_by(year, rgn_id) %>%
    summarize(trawled_area    = sum(trawl_effort * Freq) * 0.5^2,
              total_ebsa_area = sum(Freq) * 0.5^2)
  
  write_csv(ebsa_trawl_rgn_df, ebsa_trawl_rgn_file)
  
  DT::datatable(ebsa_trawl_rgn_df)
  
} else {
  
  git_prov(ebsa_trawl_rgn_file, filetype = 'output')
  
}

```

*** 

## Calculate trawl pressure on soft bottom (effort-weighted method)

Pull in soft bottom habitat and run again against trawl data.  Reference point will be based on highest trawl effort on any soft bottom habitat, scaled to 110% of that max value.  Soft bottom habitat will be based on BCMCA benthic class data, using classifications of sand and mud substrate.  The region ID, soft bottom area, and trawled hours are saved to the output folder.

``` {r create_bcmca_soft_bottom_raster}
soft_btm_rast_file <- file.path(dir_habs, 'bcmca_soft_bottom.tif')

if(!file.exists(soft_btm_rast_file)) {

  message('Loading benthic type layer')
  poly_benthic_bcmca_file <- file.path(dir_anx, '_raw_data', 'bcmca/d2015',
                                  'bcmca_eco_set_physical_complete/FeatureData_Physical',
                                  'BCMCA_ECO_Physical_BenthicClasses_DATA.shp')
  
  rast_base <- raster(rast_base_file)

  # rast_benthic_raw <- gdal_rast2(src = poly_benthic_bcmca_file,
  #                             rast_base = rast_base,
  #                             dst = file.path(dir_habs, 'benthic_raw_500m.tif'),
  #                             override_p4s = TRUE)
  
          
  ### process raw benthic Marxan_ID values based on:
  #     * 1000's = depth              * 10's = substrate          * 1's = BPI
  #         * 1000 <- 0 - 20 m            * 10 <- mud  (HARD?)        * 1 <- ridge
  #         * 2000 <- 20 - 50 m           * 20 <- sand                * 2 <- depression
  #         * 3000 <- 50 - 200 m          * 30 <- hard (MUD?)         * 3 <- flat
  #         * 4000 <- 200 m +             * 90 <- unknown             * 4 <- slope
  # NOTE THAT THE SUBSTRATE VALUES MISMATCH IN THE BCMCA METADATA... VALUES IN PARENS SEEM MORE LIKELY
  
  rast_benthic_raw <- raster(file.path(dir_habs, 'benthic_raw_500m.tif'))
  rast_soft_btm <- rast_benthic_raw - round(rast_benthic_raw, -3)
  rast_soft_btm <- round(rast_soft_btm/10) 
        
  ### Filter the benthic type layer to appropriate soft bottom cells
  # values(rast_soft_btm)[!values(rast_soft_btm) %in% c(1, 2)] <- NA ### benthic type 1 and 2 are mud and sand substrate; 3 is hard, 9 is unknown WRONG!!!?
  values(rast_soft_btm)[!values(rast_soft_btm) %in% c(2, 3)] <- NA ### benthic type 2 and 3 are sand and mud substrate; 1 is hard, 9 is unknown
  
  # rast_soft_btm <- (rast_soft_btm/rast_soft_btm) ### convert all soft-btm values to 1

  writeRaster(rast_soft_btm, soft_btm_rast_file, overwrite = TRUE)
    
} else {
  message('Soft bottom raster already exists at: \n  ', soft_btm_rast_file)
}

```

``` {r get_chi_softbottom_raster, eval = FALSE}
soft_btm_rast_file <- file.path(dir_habs, 'chi_soft_bottom.tif')

if(!file.exists(soft_btm_rast_file)) {
  ### reproject soft bottom based on global soft bottom Mollweide raster
  rast_base <- raster(rast_base_file)

  soft_btm_rast <- raster(file.path(dir_M, 'model/GL-NCEAS-Habitats_v2013a/data/soft_bottom_mol.tif'))
  soft_btm_rast_bcalb <- projectRaster(soft_btm_rast, rast_base, 
                           filename = soft_btm_rast_file,
                           overwrite = TRUE)
}

```

``` {r crosstab_soft_bottom_habs_to_rgns}

soft_btm_rast_file <- file.path(dir_habs, 'bcmca_soft_bottom.tif')
soft_btm_rast      <- raster(soft_btm_rast_file)

sb_trawl_rgn_file <- file.path(dir_goal, 'output', 'hab_soft_btm_trawl.csv')

trawl_total_files <- list.files(file.path(dir_trawl), 
                               pattern = 'trawl_totals',
                               full.names = TRUE)


### now crosstab soft bottom trawl effort by regions

trawl_totals <- lapply(trawl_total_files, raster) %>%
  setNames(basename(trawl_total_files) %>% str_replace('.tif$', ''))
  ### need a list or vector for mclapply; stack is treated as one obj?

if(!file.exists(sb_trawl_rgn_file) | reload) {
  
  ### mask by the soft bottom areas
  sb_trawl_rasts <- trawl_totals %>%
    parallel::mclapply(FUN = function(x) {
                         mask(x, soft_btm_rast)}, 
                       mc.cores = 12)
  
  rgn_rast <- raster(rast_base_file) %>%
    setNames('rgn_id')
  
  sb_trawl_rgn_list <- sb_trawl_rasts %>%
    parallel::mclapply(mc.cores = 12,
      FUN = function(sb_trawl_rast) { ### sb_trawl_rast <- sb_trawl_rasts[[1]]
        trawl_year <- names(sb_trawl_rast) %>%
          str_replace('trawl_totals_', '')
  
        trawl_rast <- sb_trawl_rast %>%
          setNames('trawl_effort')
        message('Crosstabulating ', names(trawl_rast), ' for ', trawl_year)
        trawl_df <- raster::crosstab(trawl_rast, rgn_rast,
                                    digits = 4,
                                    long = TRUE,
                                    useNA = TRUE,
                                    progress = 'text')
        trawl_df <- trawl_df %>%
          mutate(trawl_effort = as.numeric(as.character(trawl_effort)), ### dammit factors!
                 year = as.integer(trawl_year))
      })
  
  message('binding cross-tabbed dataframes from each year of trawl effort')
  sb_trawl_rgn_df <- bind_rows(sb_trawl_rgn_list) %>%
    filter(!is.na(rgn_id) & !is.na(trawl_effort)) %>% ### exclude non-EBSA cells
    mutate(max_hr_area = max(trawl_effort)) %>%
    group_by(year, rgn_id) %>%
    summarize(sum_trawl_hr_area = sum(trawl_effort * Freq) * 0.5^2, ### hours across all km^2 areas
              total_sb_area = sum(Freq) * 0.5^2,                    ### total km^2
              mean_hr_area  = sum_trawl_hr_area / total_sb_area,    ### hours per km^2
              max_hr_area   = first(max_hr_area) * .5^2)            ### maximum hours in any km^2 area
  
  write_csv(sb_trawl_rgn_df, sb_trawl_rgn_file)
  
} else {
  
  git_prov(sb_trawl_rgn_file)
  
}

```


*** 

With the trawl effort on soft-bottom habitats by region, determine scores based on log of trawl effort:  this amplifies the effects of small trawl effort relative to large trawl effort.  This suggests that damage due to each additional unit of effort does not have the same ecological impact as those first few hours of trawling effort.  Reference point (110% of max trawl effort in all soft-bottom habitat within EEZ)

``` {r calc_scores_soft_btm_trawl, eval = FALSE}

### read dataframe of ebsa trawl effort by region
sb_trawl_rgn_file <- file.path(dir_goal, 'int', 'soft_btm_trawl_rgn_df.csv')
sb_trawl_rgn_df <- read_csv(sb_trawl_rgn_file) %>%
  mutate(trawl_effort_log = log(trawl_effort + 1))

sb_trawl_effort_ref <- sb_trawl_rgn_df$trawl_effort_log %>%
  max(na.rm = TRUE) * 1.10

### Calculate area-weighted mean pressure
sb_trawl_rgn_sum <- sb_trawl_rgn_df %>%
  filter(!is.na(trawl_effort_log) &!is.na(rgn_id)) %>% ### this eliminates non-soft-btm cells (NA = non-soft-btm)
  mutate(effort_area = trawl_effort_log * Freq) %>%
  group_by(rgn_id, year) %>%
  summarize(area_sb_rgn = sum(Freq) * .25, ### each cell is .25 km^2
            mean_effort = mean(effort_area)/sum(Freq),
            mean_effort_rescale = mean_effort/sb_trawl_effort_ref,
            hab_status = 1 - mean_effort_rescale)

write_csv(sb_trawl_rgn_sum, file.path(dir_goal, 'int', 'soft_btm_trawl_rgn_summary.csv'))

# x <- read_csv(file.path(dir_goal, 'int', 'soft_btm_trawl_rgn_summary.csv'))

```

***

``` {r prov_footer, results = 'asis'}
prov_wrapup()
```

